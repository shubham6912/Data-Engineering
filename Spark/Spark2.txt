Hadoop YARN is the best resource manager world wide

RDD are immutable

100 times faster than the hadoop

Ultimately RDDs will get divided into partitions

Difference between RDD and Data Frame

Graphics -> Hierarchial

Do not compare Spark of partition with that of Hive

Like data was divided in blocks same way spark is diving the data in multiple chunks callled partitions.

It is pure logical way 

colease and repartition are two most important interview questions

How we can achive parallism using spark?

If we have already data in partioned way like hdfs for each block those many number of partitons will get created

Similary we  have a concept of group partition in spark we can use that as well

Lazy evaluation means untill an action is called data will not loaded into memory and no creation of the RDD will happen untill then

1 task means execution of 1 partition on 1 core


Job = Action = new DAG

Creating new DAG means new action


New stage will be on Wide partition not narrow with data shuffling


Based on data that will come in year we need to plan the infrastructure

How inmemory thing happens for YARN as a resource manager as spark is known for inmemory and YARN for data node resource allocation

That means data node also will have ram?
yes answerd in  Q & N a








Lazy evaluation means untill an action is called data will not loaded into memory and no creation of the RDD will happen untill then

1 task means execution of 1 partition on 1 core


Mostly Spark is integrated with YARN.


AutoScalable way is for realtime under kubernetes

Kaggle for data set

In github create architectural diagram as well as use run book to showcase your work.

Spark Context every programming lanugae has a concept of Context.

Companies do data analysis for weeks 2 to 3 before starting the dev

Data Skewness

repartion and colease are two most important

colease for writing multiple partion in one single file

Where does heap memory gets stored in the JVM??

Make a calculator for spark data execution

How many executors will be created each node and their memory who will decide?

90% time faliure will be of OOM for Executors faliure

Schedulers and DAG data structures 

Optimization we can do code side as well as configuration side

Code and infra optimization should go hand in hand

Try to experimemt on GCP with all the possible fine tuning .

Cost saving things will always be benificial

Spark dynamic allocation

Kubernetes ,
Docker

Saving money in cloud

Spark works best with Parquet file format.

Like HIVE for ORC

5 core is recommened from spark in official documentataion,

however it is not a compusion

Off heap memory will be like for Garbage Collections and JVM











